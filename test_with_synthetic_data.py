#!/usr/bin/env python3
"""
Test the EPI framework using synthetic data to ensure everything works correctly.
"""

import numpy as np
import pandas as pd
import os
import sys
from pathlib import Path

# Add the current directory to Python path to import our modules
sys.path.append(str(Path(__file__).parent))

def test_synthetic_data_loading():
    """Test that synthetic data can be loaded correctly."""
    print("üî¨ Testing synthetic data loading...")

    try:
        import h5py

        # Test loading the main synthetic file
        data_file = "synthetic_data/EPI_heterodata_diag500_synthetic.h5"

        if not os.path.exists(data_file):
            print(f"‚ùå Synthetic data file not found: {data_file}")
            return False

        with h5py.File(data_file, 'r') as f:
            x = f['x'][:]
            edge_index = f['edge_index'][:]
            edge_attr = f['edge_attr'][:]
            y = f['y'][:]

        print(f"   ‚úÖ Loaded synthetic data successfully")
        print(f"   üìä Nodes: {x.shape[0]:,}, Edges: {edge_index.shape[1]:,}")
        print(f"   üéØ Labels: {len(y):,} patients, {np.sum(y):,} positive ({100*np.mean(y):.1f}%)")

        return True

    except Exception as e:
        print(f"‚ùå Error loading synthetic data: {e}")
        return False

def test_baseline_models():
    """Test baseline model training with synthetic data."""
    print("ü§ñ Testing baseline models with synthetic data...")

    try:
        # Generate simple synthetic dataset for quick testing
        from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.linear_model import LogisticRegression
        from sklearn.svm import SVC
        from sklearn.neural_network import MLPClassifier
        from sklearn.model_selection import cross_val_score
        from sklearn.preprocessing import StandardScaler

        # Create a simple test dataset
        np.random.seed(42)
        n_samples, n_features = 1000, 50
        X = np.random.randn(n_samples, n_features)

        # Create realistic decision boundary
        weights = np.random.randn(n_features) * 0.1
        linear_combination = X @ weights
        probabilities = 1 / (1 + np.exp(-linear_combination))
        y = np.random.binomial(1, probabilities)

        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        print(f"   üìä Test dataset: {n_samples} samples, {n_features} features")
        print(f"   üéØ Class balance: {np.mean(y):.1%} positive")

        # Test each model
        models = {
            'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),
            'Random Forest': RandomForestClassifier(n_estimators=10, random_state=42, max_depth=5),
            'Logistic Regression': LogisticRegression(random_state=42, max_iter=100),
            'Gradient Boosting': GradientBoostingClassifier(n_estimators=10, random_state=42, max_depth=3),
            'SVM': SVC(random_state=42, max_iter=100),
            'MLP': MLPClassifier(hidden_layer_sizes=(20,), random_state=42, max_iter=100)
        }

        results = {}
        for name, model in models.items():
            try:
                # Quick 3-fold CV test
                scores = cross_val_score(model, X_scaled, y, cv=3, scoring='roc_auc')
                mean_score = np.mean(scores)
                results[name] = mean_score
                print(f"   ‚úÖ {name}: AUC = {mean_score:.3f} ¬± {np.std(scores):.3f}")
            except Exception as e:
                print(f"   ‚ùå {name}: Error - {e}")
                results[name] = None

        # Check if most models ran successfully
        successful_models = sum(1 for score in results.values() if score is not None)
        total_models = len(models)

        print(f"   üìä {successful_models}/{total_models} models ran successfully")

        return successful_models >= 4  # At least 4/6 models should work

    except Exception as e:
        print(f"‚ùå Error testing baseline models: {e}")
        return False

def test_progress_tracker():
    """Test the progress tracking functionality."""
    print("üìà Testing progress tracker...")

    try:
        # Import the progress tracker (we need to handle the path)
        progress_tracker_path = None
        possible_paths = [
            "experiment_progress_tracker.py",
            "scripts/utilities/progress_tracker.py"
        ]

        for path in possible_paths:
            if os.path.exists(path):
                progress_tracker_path = path
                break

        if not progress_tracker_path:
            print("   ‚ö†Ô∏è Progress tracker not found, skipping test")
            return True

        # Test creating a progress tracker
        import importlib.util
        spec = importlib.util.spec_from_file_location("progress_tracker", progress_tracker_path)
        progress_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(progress_module)

        # Create a test tracker
        tracker = progress_module.ProgressTracker(base_dir="/tmp/test_progress")
        tracker.log_message("üß™ Testing progress tracker")
        tracker.update_status("testing", 1, 3, {"test": "successful"})

        print("   ‚úÖ Progress tracker works correctly")
        return True

    except Exception as e:
        print(f"   ‚ö†Ô∏è Progress tracker test failed: {e}")
        return True  # Non-critical failure

def test_data_structure_compatibility():
    """Test that synthetic data structure matches expected format."""
    print("üîç Testing data structure compatibility...")

    try:
        import h5py

        # Load synthetic data
        with h5py.File("synthetic_data/EPI_heterodata_diag500_synthetic.h5", 'r') as f:
            x = f['x'][:]
            edge_index = f['edge_index'][:]
            edge_attr = f['edge_attr'][:]
            y = f['y'][:]

        # Check dimensions and types
        checks = []

        # Check x (node features)
        checks.append(x.ndim == 2 and x.shape[1] == 4)  # Should be (n_nodes, 4)

        # Check edge_index
        checks.append(edge_index.ndim == 2 and edge_index.shape[0] == 2)  # Should be (2, n_edges)

        # Check edge_attr
        checks.append(edge_attr.ndim == 2 and edge_attr.shape[1] == 1)  # Should be (n_edges, 1)

        # Check y (labels)
        checks.append(y.ndim == 1)  # Should be (n_patients,)

        # Check data types
        checks.append(x.dtype == np.float32)
        checks.append(edge_index.dtype == np.int64)
        checks.append(edge_attr.dtype == np.float32)
        checks.append(y.dtype == np.int64)

        # Check value ranges
        checks.append(np.all(y >= 0) and np.all(y <= 1))  # Binary labels
        checks.append(np.all(edge_attr >= 0))  # Non-negative edge weights
        checks.append(np.max(edge_index) < len(x))  # Valid edge indices

        passed_checks = sum(checks)
        total_checks = len(checks)

        print(f"   üìä Compatibility checks: {passed_checks}/{total_checks} passed")

        if passed_checks >= total_checks - 1:  # Allow one minor failure
            print("   ‚úÖ Data structure is compatible")
            return True
        else:
            print("   ‚ùå Data structure compatibility issues found")
            return False

    except Exception as e:
        print(f"‚ùå Error testing data structure: {e}")
        return False

def run_comprehensive_test():
    """Run all tests and provide a summary."""
    print("üß™ Running comprehensive test suite...")
    print("=" * 60)

    tests = [
        ("Synthetic Data Loading", test_synthetic_data_loading),
        ("Data Structure Compatibility", test_data_structure_compatibility),
        ("Baseline Models", test_baseline_models),
        ("Progress Tracker", test_progress_tracker),
    ]

    results = {}
    for test_name, test_func in tests:
        print(f"\nüî¨ Running: {test_name}")
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"‚ùå Test '{test_name}' failed with error: {e}")
            results[test_name] = False

    print("\n" + "=" * 60)
    print("üìä TEST SUMMARY")
    print("=" * 60)

    passed = 0
    total = len(tests)

    for test_name, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status:8} {test_name}")
        if result:
            passed += 1

    print(f"\nüèÜ Overall Result: {passed}/{total} tests passed")

    if passed == total:
        print("üéâ All tests passed! The framework is ready for use.")
        return True
    elif passed >= total * 0.75:
        print("‚ö†Ô∏è Most tests passed. Framework should work with minor issues.")
        return True
    else:
        print("‚ùå Multiple test failures. Please check the setup.")
        return False

if __name__ == "__main__":
    success = run_comprehensive_test()
    exit(0 if success else 1)