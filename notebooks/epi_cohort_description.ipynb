{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":740},"executionInfo":{"elapsed":40117,"status":"ok","timestamp":1736223527973,"user":{"displayName":"Alex Han","userId":"14584877627806570126"},"user_tz":360},"id":"HzDspD47tD4g","outputId":"9fe4e0f5-cfe3-4ced-f1d4-fa5d1040ad1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [61.1 kB]\n","Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,199 kB]\n","Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,631 kB]\n","Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,552 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,854 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,513 kB]\n","Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,652 kB]\n","Fetched 19.6 MB in 4s (5,036 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","52 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n","tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n","Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"]},{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7bd4c0b3a890>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://b6f64c7001ea:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Our First Spark Example</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":2}],"source":["!sudo apt update\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n","!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n","!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n","!pip install -q findspark\n","!pip install pyspark\n","!pip install py4j\n","\n","import os\n","import sys\n","# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n","\n","\n","import findspark\n","findspark.init()\n","findspark.find()\n","\n","import pyspark\n","\n","from pyspark.sql import DataFrame, SparkSession\n","from typing import List\n","import pyspark.sql.types as T\n","import pyspark.sql.functions as F\n","\n","spark= SparkSession \\\n","       .builder \\\n","       .appName(\"Our First Spark Example\") \\\n","       .getOrCreate()\n","\n","spark"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_sx5VhZ7v8GK","executionInfo":{"status":"ok","timestamp":1736223802522,"user_tz":360,"elapsed":592,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}}},"outputs":[],"source":["import pandas as pd\n","# import pandasql as ps\n","# # import pixiedust\n","# import sys\n","# sys.path.append('..')\n","# import util\n","# import etl\n","# import pyarrow.parquet as pq\n","# import pyarrow as pa\n","# !pip install duckdb\n","# import duckdb\n","from pyspark.sql.functions import *\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import *\n","# spark = SparkSession.builder.getOrCreate()\n","\n","# util.usedatabase(spark, \"real_world_data_jun_2022\")\n","\n","# pixiedust.enableJobMonitor()\n","\n","# con = duckdb.connect()"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18096,"status":"ok","timestamp":1736223487860,"user":{"displayName":"Alex Han","userId":"14584877627806570126"},"user_tz":360},"id":"5W8ujVoVZNkR","outputId":"bbd265e4-764c-437c-f0c6-a3f77def9e0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gGkRWJw6v8GT","executionInfo":{"status":"ok","timestamp":1736223810425,"user_tz":360,"elapsed":5697,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}}},"outputs":[],"source":["cohort_new = spark.read.parquet('/content/drive/MyDrive/UCO/projects/epilepsy/allcohort2024')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rK3Delyfv8GU","executionInfo":{"status":"ok","timestamp":1736223812691,"user_tz":360,"elapsed":2267,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}}},"outputs":[],"source":["cohort_demo = spark.read.parquet('/content/drive/MyDrive/UCO/projects/epilepsy/allcohort_new_demo')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4702,"status":"ok","timestamp":1729016737997,"user":{"displayName":"Alex Han","userId":"14584877627806570126"},"user_tz":300},"id":"-WPkDjHZv8GU","outputId":"2f7bc0bb-fbe8-4ddd-e167-95c9982f3505"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1701685"]},"metadata":{},"execution_count":6}],"source":["cohort_demo.count()\n","# 1701685"]},{"cell_type":"code","source":["cohort_demo.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZG70UwkJAQAx","executionInfo":{"status":"ok","timestamp":1736223835118,"user_tz":360,"elapsed":5312,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}},"outputId":"68431b2a-9577-43b8-b841-fc044abee6fc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+--------------------+------+--------------------+-----+\n","|            personid|           birthdate|gender|                date| race|\n","+--------------------+--------------------+------+--------------------+-----+\n","|00000559-6f93-4e9...|{[1970-02-19], 19...|Female|2017-02-02T19:29:...|White|\n","|00003900-2614-4da...|{[2009-09-05], 20...|  Male|2015-09-27T04:00:...|White|\n","|0001df3b-fec4-4e9...|{[1984-04-01], 19...|Female| 2016-03-01T00:00:00|White|\n","|000538ff-d137-48e...|{[1974-10-16], 19...|  Male|2017-06-22T05:00:...| NULL|\n","|001517b6-1588-409...|{[2015-02-10], 20...|  Male| 2020-07-27T00:00:00|White|\n","+--------------------+--------------------+------+--------------------+-----+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXNNCekFv8GU"},"outputs":[],"source":["cohort_como = spark.read.parquet('/content/drive/MyDrive/UCO/projects/epilepsy/allcohort_new_como')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2111,"status":"ok","timestamp":1729016740490,"user":{"displayName":"Alex Han","userId":"14584877627806570126"},"user_tz":300},"id":"ncyccVzjv8GU","outputId":"aecf6011-0d9c-4fc7-c800-5ecc305011c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+--------------------+-------------+--------------------+\n","|            personid|         encounterid|comorbidityid|       effectivedate|\n","+--------------------+--------------------+-------------+--------------------+\n","|00169bfe-c751-491...|fd5f3f12-37ca-418...|       Z34.82|2022-03-21T12:44:...|\n","|00169bfe-c751-491...|f42c4077-f0cf-469...|        Z33.1|2021-11-28T20:07:...|\n","|00169bfe-c751-491...|f42c4077-f0cf-469...|        U07.1|2021-11-28T20:13:...|\n","|00169bfe-c751-491...|f42c4077-f0cf-469...|        Z45.2|2022-01-04T15:02:...|\n","|00169bfe-c751-491...|f42c4077-f0cf-469...|        E04.9|2022-01-02T17:00:...|\n","+--------------------+--------------------+-------------+--------------------+\n","only showing top 5 rows\n","\n"]}],"source":["cohort_como.show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NoOiHnoav8GU"},"outputs":[],"source":["cohort_como_reformat= cohort_como.withColumn('comorbidityid',expr(\"regexp_replace(comorbidityid, '([0-9a-zA-Z]+\\\\.[0-9a-zA-Z]).*', '$1')\").cast(\"string\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1729016741000,"user":{"displayName":"Alex Han","userId":"14584877627806570126"},"user_tz":300},"id":"zbJ_qxz8v8GU","outputId":"19e8fd58-76bb-4923-8a1c-e381c0a613e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+--------------------+-------------+--------------------+\n","|            personid|         encounterid|comorbidityid|       effectivedate|\n","+--------------------+--------------------+-------------+--------------------+\n","|00169bfe-c751-491...|fd5f3f12-37ca-418...|        Z34.8|2022-03-21T12:44:...|\n","|00169bfe-c751-491...|f42c4077-f0cf-469...|        Z33.1|2021-11-28T20:07:...|\n","|00169bfe-c751-491...|f42c4077-f0cf-469...|        U07.1|2021-11-28T20:13:...|\n","|00169bfe-c751-491...|f42c4077-f0cf-469...|        Z45.2|2022-01-04T15:02:...|\n","|00169bfe-c751-491...|f42c4077-f0cf-469...|        E04.9|2022-01-02T17:00:...|\n","+--------------------+--------------------+-------------+--------------------+\n","only showing top 5 rows\n","\n"]}],"source":["cohort_como_reformat.show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSauLPhVv8GV"},"outputs":[],"source":["def get_top_comorbidity(spark, como, topn):\n","    top_como = como.groupBy('comorbidityid').count().orderBy(col('count').desc())\\\n","        .select(collect_list('comorbidityid')).collect()[0]\\\n","            .__getitem__('collect_list(comorbidityid)')[0:topn]\n","\n","#     top_como_coded = [\"n\"+x.replace('.','_') for x in top_como]\n","#     out = {top_como[i]:top_como_coded[i] for i in range(len(top_como))} # store in a dictionary\n","    print(top_como)\n","    return top_como"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13158,"status":"ok","timestamp":1729016754155,"user":{"displayName":"Alex Han","userId":"14584877627806570126"},"user_tz":300},"id":"P9piI7oyv8GV","outputId":"9274018c-8528-4b0b-df5c-5cf78c20ded5"},"outputs":[{"output_type":"stream","name":"stdout","text":["['I10', 'Z79.8', 'Z00.1', 'Z23', 'M25.5', 'E78.5', 'E11.9', 'W19.X', 'J06.9', 'I25.1', 'E03.9', 'K21.9', 'Y92.0', 'Z79.0', 'F41.9', '401.9', 'R51', 'R05', 'I48.9', 'M79.6', 'Z00.0', 'F32.9', 'R50.9', 'J45.9', 'R10.9', 'V20.2', 'Z87.8', 'N39.0', 'V58.6', 'J02.9', 'S00.8', 'R42', 'E78.0', 'F17.2', 'Z86.7', 'D64.9', '272.4', 'R55', 'R06.0', '789.0', '250.0', 'M54.2', 'M54.5', 'S01.8', 'R07.9', 'K59.0', '465.9', 'R53.8', 'S01.0', 'R11.1']\n"]}],"source":["top_comorbidity = get_top_comorbidity(spark, cohort_como_reformat, topn=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhqP-mX4v8GV"},"outputs":[],"source":["cohort_como_length = cohort_como_reformat.join(cohort_new.select('personid','date'),'personid').withColumn('len_como',months_between(col('date'),col('effectivedate'))/12)"]},{"cell_type":"code","source":["# prompt: make the len_como integer\n","\n","cohort_como_length = cohort_como_length.withColumn(\"len_como\", col(\"len_como\").cast(\"integer\"))\n"],"metadata":{"id":"VzuSjvQX2reB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5IxfAnkv8GW"},"outputs":[],"source":["cohort_como_length_filter = cohort_como_length.filter(col('comorbidityid').isin(top_comorbidity))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22130,"status":"ok","timestamp":1729016776585,"user":{"displayName":"Alex Han","userId":"14584877627806570126"},"user_tz":300},"id":"HWOVape2aFeB","outputId":"cd355dd3-ab69-4ff6-a408-d5e3617cc59b"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+--------------------+-------------+--------------------+--------------------+--------+\n","|            personid|         encounterid|comorbidityid|       effectivedate|                date|len_como|\n","+--------------------+--------------------+-------------+--------------------+--------------------+--------+\n","|00010e6e-593a-4e1...|d60bc852-0992-4b1...|        V58.6|2015-08-31T09:06:...|2020-03-17T03:09:...|       4|\n","|00010e6e-593a-4e1...|c8a0da6a-aadd-47d...|        M79.6|2016-01-29T04:15:...|2020-03-17T03:09:...|       4|\n","|00010e6e-593a-4e1...|1fbe46d9-25ff-467...|        M54.2|2020-01-14T17:00:...|2020-03-17T03:09:...|       0|\n","|00010e6e-593a-4e1...|87247b0f-a8f0-45d...|        F17.2|2020-02-28T19:30:...|2020-03-17T03:09:...|       0|\n","|00010e6e-593a-4e1...|87247b0f-a8f0-45d...|        M54.2|2020-02-28T19:30:...|2020-03-17T03:09:...|       0|\n","+--------------------+--------------------+-------------+--------------------+--------------------+--------+\n","only showing top 5 rows\n","\n"]}],"source":["cohort_como_length_filter.show(5)"]},{"cell_type":"code","source":["# prompt: keep personid, comorbidityid and len_como in cohort_como_length_filter and remove duplicates\n","\n","cohort_como_length_filter_selected = cohort_como_length_filter.select('personid', 'comorbidityid', 'len_como').dropDuplicates()\n","# cohort_como_length_filter_selected.show(5)\n"],"metadata":{"id":"h4CM99Vw3MKD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bPwJpDIv8GW","outputId":"efdb4973-0a20-4564-f26a-ae2c6306d1f4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729016791600,"user_tz":300,"elapsed":15019,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["11159560"]},"metadata":{},"execution_count":18}],"source":["cohort_como_length_filter.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZugSWmeMv8Ga","outputId":"3956d8d4-b640-4b8f-a48d-c6360e105268","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729016791755,"user_tz":300,"elapsed":167,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+--------------------+------+--------------------+-----+\n","|            personid|           birthdate|gender|                date| race|\n","+--------------------+--------------------+------+--------------------+-----+\n","|00000559-6f93-4e9...|{[1970-02-19], 19...|Female|2017-02-02T19:29:...|White|\n","|00003900-2614-4da...|{[2009-09-05], 20...|  Male|2015-09-27T04:00:...|White|\n","|0001df3b-fec4-4e9...|{[1984-04-01], 19...|Female| 2016-03-01T00:00:00|White|\n","|000538ff-d137-48e...|{[1974-10-16], 19...|  Male|2017-06-22T05:00:...| NULL|\n","|001517b6-1588-409...|{[2015-02-10], 20...|  Male| 2020-07-27T00:00:00|White|\n","+--------------------+--------------------+------+--------------------+-----+\n","only showing top 5 rows\n","\n"]}],"source":["cohort_demo.show(5)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"-pA_nZ4Rv8Ga","executionInfo":{"status":"ok","timestamp":1736223859906,"user_tz":360,"elapsed":275,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}}},"outputs":[],"source":["cohort_demo_processed = cohort_demo.withColumn('age_at_diagnosis',months_between(col('date'),col('birthdate.value'))/12)\n","cohort_demo_processed = cohort_demo_processed.withColumn('age_at_diagnosis',col('age_at_diagnosis').cast(\"integer\"))\n","cohort_demo_processed = cohort_demo_processed.drop('birthdate')\n","# cohort_demo_processed.show(5)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":794,"status":"ok","timestamp":1736223861553,"user":{"displayName":"Alex Han","userId":"14584877627806570126"},"user_tz":360},"id":"Zr7sXGjCZRsm","outputId":"18584830-d110-48b9-b804-ab2a9f1a8dd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+------+--------------------+----+----------------+\n","|            personid|gender|                date|race|age_at_diagnosis|\n","+--------------------+------+--------------------+----+----------------+\n","|00000559-6f93-4e9...|     2|2017-02-02T19:29:...|   1|              46|\n","|00003900-2614-4da...|     1|2015-09-27T04:00:...|   1|               6|\n","|0001df3b-fec4-4e9...|     2| 2016-03-01T00:00:00|   1|              31|\n","|000538ff-d137-48e...|     1|2017-06-22T05:00:...|   0|              42|\n","|001517b6-1588-409...|     1| 2020-07-27T00:00:00|   1|               5|\n","|00179a60-4542-4d4...|     1|2020-03-27T11:35:...|   1|              81|\n","|001a2011-9523-415...|     1|2022-01-08T12:11:...|   1|              26|\n","|0021fe8a-5e63-441...|     1|2019-11-28T18:00:...|   1|               2|\n","|0021ff9b-1406-472...|     1|2018-01-11T08:00:...|   1|              26|\n","|0029b094-5715-477...|     2|                    |   2|            NULL|\n","|00384f92-4a1a-4e7...|     1|2020-04-14T18:26:...|   1|              36|\n","|003bd84f-f6bd-40a...|     2|2021-08-28T09:33:...|   1|              17|\n","|003eb796-70b0-430...|     2| 2019-09-23T00:00:00|   1|               1|\n","|00419280-7519-418...|     1|2020-09-28T23:14:...|   1|              37|\n","|004ceaa1-55fe-4a8...|     2|2017-10-02T17:00:...|   1|               1|\n","|006adaf8-fbbd-463...|     2|2020-12-22T16:22:...|   1|              14|\n","|006ce9a6-a417-4dd...|     2|                    |   1|            NULL|\n","|0074c1d2-0127-4a3...|     2|                    |   1|            NULL|\n","|00775ada-13ae-47f...|     2|2019-07-16T17:00:...|   1|               2|\n","|007b3e6e-07d0-452...|     1|2014-06-10T05:00:...|   1|              11|\n","+--------------------+------+--------------------+----+----------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Preprocess gender column\n","df = cohort_demo_processed\n","df = df.withColumn(\"gender\",\n","                   when(lower(col(\"gender\")).isin(\"male\"), 1)\n","                   .when(lower(col(\"gender\")).isin(\"female\"), 2)\n","                   .when(col(\"gender\").isNull(), 0)\n","                   .otherwise(3))\n","\n","# Preprocess race column\n","df = df.withColumn(\"race\",\n","                   when(lower(col(\"race\")).like(\"%white%\") | lower(col(\"race\")).like(\"%caucasian%\"), 1)\n","                   .when(lower(col(\"race\")).like(\"%black%\") | lower(col(\"race\")).like(\"%african%\"), 2)\n","                   .when(lower(col(\"race\")).like(\"%hispanic%\"), 3)\n","                   .when(lower(col(\"race\")).like(\"%asian%\") | lower(col(\"race\")).like(\"%chinese%\") | lower(col(\"race\")).like(\"%korean%\") | lower(col(\"race\")).like(\"%japanese%\"), 4)\n","                   .when(lower(col(\"race\")).like(\"%indian american%\") | lower(col(\"race\")).like(\"%native american%\"), 5)\n","                   .when(col(\"race\").isNull(), 0)\n","                   .otherwise(6))\n","df.show()\n","cohort_demo_processed = df"]},{"cell_type":"code","source":["# prompt: join with cohort new\n","\n","# Assuming 'cohort_new' DataFrame already exists as defined in the previous code.\n","\n","# Assuming 'cohort_demo_processed' DataFrame already exists as defined in the previous code.\n","\n","# Join the two dataframes\n","joined_cohort = cohort_new.join(cohort_demo_processed, on='personid', how='inner')\n","\n","# Show the first 5 rows of the joined dataframe\n","joined_cohort.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BL-nCaLLAsnp","executionInfo":{"status":"ok","timestamp":1736224012691,"user_tz":360,"elapsed":56653,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}},"outputId":"f73dd3ed-2494-4f63-9f4f-38c9bbc430b9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+--------------------+---+------+--------------------+----+----------------+\n","|            personid|                date|EPI|gender|                date|race|age_at_diagnosis|\n","+--------------------+--------------------+---+------+--------------------+----+----------------+\n","|000084f6-14ea-48c...|2017-06-17T07:00:...|  0|     1|2017-06-17T07:00:...|   6|              38|\n","|00010e6e-593a-4e1...|2020-03-17T03:09:...|  0|     2|2020-03-17T03:09:...|   1|              31|\n","|00012328-6d23-405...|2020-05-11T19:00:...|  0|     2|2020-05-11T19:00:...|   1|               2|\n","|000151d8-40ee-46d...|2021-09-22T18:40:...|  0|     1|2021-09-22T18:40:...|   1|              77|\n","|0001b472-d009-41b...|2021-06-28T16:00:...|  0|     2|2021-06-28T16:00:...|   2|               1|\n","+--------------------+--------------------+---+------+--------------------+----+----------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["joined_cohort = joined_cohort.select('personid','age_at_diagnosis','gender','race','EPI')"],"metadata":{"id":"sImb0J1BD0X3","executionInfo":{"status":"ok","timestamp":1736224773146,"user_tz":360,"elapsed":254,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# prompt: for age_at_diagnosis, describe the mean, and 95% confidence interval of age_at_diagnosis; For gender, describe percentile of 0, 1, 2, ; for for race describe percentile of 1, 2, 3, 4, 5, 6.  stratified by different EPI groups (0 or 1)\n","\n","from pyspark.sql.functions import mean, stddev, count, percentile_approx\n","\n","# Group by EPI group (assuming 'EPI' column exists)\n","grouped_cohort = joined_cohort.groupBy(\"EPI\")\n","\n","# Calculate mean and standard deviation of age_at_diagnosis for each EPI group\n","summary_stats = grouped_cohort.agg(\n","    mean(\"age_at_diagnosis\").alias(\"mean_age\"),\n","    stddev(\"age_at_diagnosis\").alias(\"stddev_age\"),\n","    count(\"*\").alias(\"n\")\n",")\n","\n","# Calculate the 95% confidence interval\n","summary_stats = summary_stats.withColumn(\n","    \"lower_ci\", summary_stats[\"mean_age\"] - 1.96 * (summary_stats[\"stddev_age\"] / (summary_stats[\"n\"] ** 0.5))\n",").withColumn(\n","    \"upper_ci\", summary_stats[\"mean_age\"] + 1.96 * (summary_stats[\"stddev_age\"] / (summary_stats[\"n\"] ** 0.5))\n",")\n","\n","summary_stats.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRT4pvtXAcVq","executionInfo":{"status":"ok","timestamp":1736225195576,"user_tz":360,"elapsed":5392,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}},"outputId":"d2494c7e-8ed7-438d-d5d0-b4410297e83f"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-----------------+------------------+-------+-----------------+------------------+\n","|EPI|         mean_age|        stddev_age|      n|         lower_ci|          upper_ci|\n","+---+-----------------+------------------+-------+-----------------+------------------+\n","|  1|43.52829781261472|23.939486199073553| 102687|43.38187351028304| 43.67472211494639|\n","|  0|32.20221205268794|27.451904461337097|1598998|32.15966156031688|32.244762545059004|\n","+---+-----------------+------------------+-------+-----------------+------------------+\n","\n"]}]},{"cell_type":"code","source":["# prompt: get the distributions of gender for different EPI\n","\n","from pyspark.sql.functions import count, when\n","\n","# Assuming 'joined_cohort' DataFrame already exists as defined in the previous code.\n","\n","# Group by EPI and gender\n","gender_distribution = joined_cohort.groupBy(\"EPI\", \"gender\").agg(count(\"*\").alias(\"count\"))\n","\n","# Show the results\n","gender_distribution.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"81i7SKu2FnJw","executionInfo":{"status":"ok","timestamp":1736225269271,"user_tz":360,"elapsed":4278,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}},"outputId":"555a230c-2ff0-48dd-8f25-a9426aec43a5"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------+------+\n","|EPI|gender| count|\n","+---+------+------+\n","|  1|     0|    15|\n","|  1|     2| 44068|\n","|  1|     1| 58516|\n","|  1|     3|    88|\n","|  0|     0|   440|\n","|  0|     1|854020|\n","|  0|     2|743119|\n","|  0|     3|  1419|\n","+---+------+------+\n","\n"]}]},{"cell_type":"code","source":["# prompt: generate race distribution, sort by EPI then race\n","\n","# Assuming 'joined_cohort' DataFrame already exists as defined in the previous code.\n","\n","# Group by EPI and race\n","race_distribution = joined_cohort.groupBy(\"EPI\", \"race\").agg(count(\"*\").alias(\"count\"))\n","\n","# Order by EPI then race\n","race_distribution = race_distribution.orderBy(\"EPI\", \"race\")\n","\n","# Show the results\n","race_distribution.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7rX9zBFFiDo","executionInfo":{"status":"ok","timestamp":1736226539060,"user_tz":360,"elapsed":3776,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}},"outputId":"5438d6e2-b80d-4fd2-dfc3-55dc47e8a2f1"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+----+-------+\n","|EPI|race|  count|\n","+---+----+-------+\n","|  0|   0|  26543|\n","|  0|   1|1101705|\n","|  0|   2| 165850|\n","|  0|   3|  20257|\n","|  0|   4|  34026|\n","|  0|   6| 250617|\n","|  1|   0|   2006|\n","|  1|   1|  72049|\n","|  1|   2|  12403|\n","|  1|   3|    662|\n","|  1|   4|   1190|\n","|  1|   6|  14377|\n","+---+----+-------+\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9e6sJJ2v8Ga"},"outputs":[],"source":["person_vertice = cohort_demo_processed\n","person_vertice = person_vertice.withColumn(\"id\", col('personid')).drop('personid','date')\n","diagnosis_vertice = cohort_como_length_filter.select(\"comorbidityid\").distinct()\n","diagnosis_vertice = diagnosis_vertice.withColumn(\"id\", col('comorbidityid')).drop(col('comorbidityid'))"]},{"cell_type":"code","source":["person_vertice.select('id').count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oEQ7_pjIM4wQ","executionInfo":{"status":"ok","timestamp":1729016794371,"user_tz":300,"elapsed":2010,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}},"outputId":"e66c46de-5c79-4650-ddf0-3f3bf67b3e5d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1701685"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["person_vertice.select('id').distinct().count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4923rzYnM_Ua","executionInfo":{"status":"ok","timestamp":1729016797680,"user_tz":300,"elapsed":3312,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}},"outputId":"0b1870cb-2dea-4fe9-ecaf-70c73df36ea7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1701685"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0U5pEspNv8Ga"},"outputs":[],"source":["# print(person_vertice.columns)\n","# print(diagnosis_vertice.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8VTx3GQBv8Ga"},"outputs":[],"source":["# def union_columns(df1, df2):\n","#     diff1 = [c for c in df2.columns if c not in df1.columns]\n","#     diff2 = [c for c in df1.columns if c not in df2.columns]\n","#     df = df1.select('*', *[lit(None).alias(c) for c in diff1]) \\\n","#         .unionByName(df2.select('*', *[lit(None).alias(c) for c in diff2]))\n","#     return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXWhohM2v8Gb"},"outputs":[],"source":["# All_vertices = union_columns(person_vertice, diagnosis_vertice)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SD-YXh4v8Gb"},"outputs":[],"source":["personid_diagnosis_edges = cohort_new.join(cohort_como_length_filter,'personid').select(\"personid\", \"comorbidityid\",'len_como')\n","# personid_diagnosis_edges = personid_diagnosis_edges.withColumn(\"cormorbidity_year\", col('comorbidity_year'))\n","personid_diagnosis_edges = personid_diagnosis_edges.withColumn(\"src\", col('personid'))\n","personid_diagnosis_edges = personid_diagnosis_edges.withColumn(\"dst\", col('comorbidityid'))\n","\n","diagnosis_personid_edges = personid_diagnosis_edges.withColumn(\"src\", col('comorbidityid'))\n","diagnosis_personid_edges = personid_diagnosis_edges.withColumn(\"dst\", col('personid'))\n","\n","personid_diagnosis_edges = personid_diagnosis_edges.drop(col('personid')).drop(col('comorbidityid'))\n","diagnosis_personid_edges = diagnosis_personid_edges.drop(col('personid')).drop(col('comorbidityid'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1kSxHbhOVCd"},"outputs":[],"source":["# all_edges = personid_diagnosis_edges.unionAll(diagnosis_personid_edges)\n","all_edges = personid_diagnosis_edges"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18098,"status":"ok","timestamp":1729016815968,"user":{"displayName":"Alex Han","userId":"14584877627806570126"},"user_tz":300},"id":"dnNLdpGYcuP9","outputId":"5d5b5b30-dd4a-400e-f037-750c5615db27"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+--------------------+-----+\n","|len_como|                 src|  dst|\n","+--------+--------------------+-----+\n","|       4|00010e6e-593a-4e1...|V58.6|\n","|       4|00010e6e-593a-4e1...|M79.6|\n","|       0|00010e6e-593a-4e1...|M54.2|\n","|       0|00010e6e-593a-4e1...|F17.2|\n","|       0|00010e6e-593a-4e1...|M54.2|\n","+--------+--------------------+-----+\n","only showing top 5 rows\n","\n"]}],"source":["all_edges.show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-aSWVQrv8Ge"},"outputs":[],"source":["# Create input features tensor\n","# import torch\n","\n","# # Mapping person and comorbidity IDs to indices\n","# person_id_to_idx = {id_: idx for idx, id_ in enumerate(person_vertice.toPandas()['id'])}\n","# comorbidity_id_to_idx = {id_: idx + len(person_id_to_idx) for idx, id_ in enumerate(diagnosis_vertice.toPandas()['id'])}\n","# id_to_idx = {**person_id_to_idx, **comorbidity_id_to_idx}"]},{"cell_type":"code","source":["# revised to put comorbidity indexes at the beginning\n","import torch\n","\n","# Mapping person and comorbidity IDs to indices\n","comorbidity_id_to_idx = {id_: idx  for idx, id_ in enumerate(diagnosis_vertice.toPandas()['id'])}\n","person_id_to_idx = {id_: idx + len(comorbidity_id_to_idx) for idx, id_ in enumerate(person_vertice.toPandas()['id'])}\n","\n","id_to_idx = {**comorbidity_id_to_idx, **person_id_to_idx}"],"metadata":{"id":"FMzSRfjGR_S-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vIACdU4qb8c1"},"outputs":[],"source":["# prompt: convert id of person_vertice using id_to_idx\n","\n","# Assuming 'person_vertice' is a PySpark DataFrame\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import IntegerType\n","\n","# Create a UDF to map person IDs to indices\n","map_person_id_udf = udf(lambda id_: id_to_idx.get(id_), IntegerType())\n","\n","# Apply the UDF to the 'id' column of 'person_vertice'\n","person_vertice_indexed = person_vertice.withColumn(\"id_index\", map_person_id_udf(col(\"id\")))\n","\n","# Show the updated DataFrame\n","# person_vertice_indexed.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcicZP8UcPEP"},"outputs":[],"source":["# prompt: convert id of diagnosis_vertice using id_to_idx\n","\n","# Create a UDF to map comorbidity IDs to indices\n","map_comorbidity_id_udf = udf(lambda id_: id_to_idx.get(id_), IntegerType())\n","\n","# Apply the UDF to the 'id' column of 'diagnosis_vertice'\n","diagnosis_vertice_indexed = diagnosis_vertice.withColumn(\"id_index\", map_comorbidity_id_udf(col(\"id\")))\n","\n","# Show the updated DataFrame\n","# diagnosis_vertice_indexed.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNoyjZchcqUX"},"outputs":[],"source":["# prompt: convert src and dst of all_edges using id_to_idx\n","\n","# Create a UDF to map IDs to indices for 'src' column\n","map_src_id_udf = udf(lambda id_: id_to_idx.get(id_), IntegerType())\n","\n","# Apply the UDF to the 'src' column of 'all_edges'\n","all_edges_indexed = all_edges.withColumn(\"src_index\", map_src_id_udf(col(\"src\")))\n","\n","# Create a UDF to map IDs to indices for 'dst' column\n","map_dst_id_udf = udf(lambda id_: id_to_idx.get(id_), IntegerType())\n","\n","# Apply the UDF to the 'dst' column of 'all_edges_indexed'\n","all_edges_indexed = all_edges_indexed.withColumn(\"dst_index\", map_dst_id_udf(col(\"dst\")))\n","\n","# Show the updated DataFrame\n","# all_edges_indexed.show()\n"]},{"cell_type":"code","source":["person_vertice_indexed.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbt_GE5JN3H-","executionInfo":{"status":"ok","timestamp":1729016861116,"user_tz":300,"elapsed":2148,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}},"outputId":"2332fcc9-bde0-49f8-c73b-b8e4fc852f2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+----+----------------+--------------------+--------+\n","|gender|race|age_at_diagnosis|                  id|id_index|\n","+------+----+----------------+--------------------+--------+\n","|     2|   1|              46|00000559-6f93-4e9...|      50|\n","|     1|   1|               6|00003900-2614-4da...|      51|\n","|     2|   1|              31|0001df3b-fec4-4e9...|      52|\n","|     1|   0|              42|000538ff-d137-48e...|      53|\n","|     1|   1|               5|001517b6-1588-409...|      54|\n","|     1|   1|              81|00179a60-4542-4d4...|      55|\n","|     1|   1|              26|001a2011-9523-415...|      56|\n","|     1|   1|               2|0021fe8a-5e63-441...|      57|\n","|     1|   1|              26|0021ff9b-1406-472...|      58|\n","|     2|   2|            NULL|0029b094-5715-477...|      59|\n","|     1|   1|              36|00384f92-4a1a-4e7...|      60|\n","|     2|   1|              17|003bd84f-f6bd-40a...|      61|\n","|     2|   1|               1|003eb796-70b0-430...|      62|\n","|     1|   1|              37|00419280-7519-418...|      63|\n","|     2|   1|               1|004ceaa1-55fe-4a8...|      64|\n","|     2|   1|              14|006adaf8-fbbd-463...|      65|\n","|     2|   1|            NULL|006ce9a6-a417-4dd...|      66|\n","|     2|   1|            NULL|0074c1d2-0127-4a3...|      67|\n","|     2|   1|               2|00775ada-13ae-47f...|      68|\n","|     1|   1|              11|007b3e6e-07d0-452...|      69|\n","+------+----+----------------+--------------------+--------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["# prompt: convert null in age at diagnosis to 0\n","\n","person_vertice_indexed = person_vertice_indexed.fillna({'age_at_diagnosis': 0})\n"],"metadata":{"id":"W6zpK0Zz9NC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # prompt: handle null value of x_person\n","\n","# # Handle null values in x_person\n","# x_person = torch.nan_to_num(x_person, nan=0)  # Replace NaN with 0.0\n","\n","# print(x_person)\n"],"metadata":{"id":"QbiNTXwzO6tE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6c6_pBbrNo8","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1729016893878,"user_tz":300,"elapsed":32766,"user":{"displayName":"Alex Han","userId":"14584877627806570126"}},"outputId":"54001694-8894-4fff-a7fb-0b8dde21d931"},"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o371.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 68.0 failed 1 times, most recent failure: Lost task 6.0 in stage 68.0 (TID 215) (f19afb5276c5 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.smj_findNextJoinRows_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1633/0x0000000840b4c840.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.smj_findNextJoinRows_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1633/0x0000000840b4c840.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-43db1016b5c3>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Assuming 'diagnosis_vertice' is a PySpark DataFrame with column: 'id_index'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m x_diagnosis = torch.tensor(diagnosis_vertice_indexed.select(\"id_index\")\n\u001b[0;32m----> 9\u001b[0;31m                             .toPandas().values, dtype=torch.int)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# print(x_person)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             pdf = pd.DataFrame.from_records(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \"\"\"\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o371.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 68.0 failed 1 times, most recent failure: Lost task 6.0 in stage 68.0 (TID 215) (f19afb5276c5 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.smj_findNextJoinRows_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1633/0x0000000840b4c840.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.smj_findNextJoinRows_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1633/0x0000000840b4c840.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"]}],"source":["# prompt: format person_vertice to x_person, format diagnosis_vertice to x_diagnosis\n","\n","# Assuming 'person_vertice' is a PySpark DataFrame with columns: 'id_index', 'age_at_diagnosis', 'gender', 'race'\n","x_person = torch.tensor(person_vertice_indexed.select(\"id_index\", \"age_at_diagnosis\", \"gender\", \"race\")\n","                         .toPandas().values, dtype=torch.int)\n","\n","# Assuming 'diagnosis_vertice' is a PySpark DataFrame with column: 'id_index'\n","x_diagnosis = torch.tensor(diagnosis_vertice_indexed.select(\"id_index\")\n","                            .toPandas().values, dtype=torch.int)\n","\n","# print(x_person)\n","# print(x_diagnosis)\n"]},{"cell_type":"code","source":["# prompt: pad x_diagnosis with 0 so that it has the same dimensions except for dimension 0\n","\n","# Pad x_diagnosis with zeros\n","padding_size = x_person.shape[1] - x_diagnosis.shape[1]\n","padding = torch.zeros(x_diagnosis.shape[0], padding_size)\n","x_diagnosis_padded = torch.cat([x_diagnosis, padding], dim=1).float()\n","\n","print(x_diagnosis_padded)\n"],"metadata":{"id":"b0iYdxKZeEAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Merge person and diagnosis features\n","# x_combined = torch.cat([x_person, x_diagnosis_padded], dim=0)\n","# print(x_combined)"],"metadata":{"id":"AoMdF6Z_d3gd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge person and diagnosis features, put diagnosis first\n","x_combined = torch.cat([x_diagnosis_padded, x_person], dim=0)\n","print(x_combined)"],"metadata":{"id":"R4wEZfVKSigz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NYXVmZIadDEq"},"outputs":[],"source":["# prompt: format all_edges to the structure of edge_index_person_to_diagnosis, the edge_attr will be len_como\n","\n","# Extract source and destination indices and edge attributes\n","src_indices = all_edges_indexed.select(\"src_index\").rdd.flatMap(lambda x: x).collect()\n","# dst_indices = all_edges_indexed.select(\"dst_index\").rdd.flatMap(lambda x: x).collect()\n","# edge_attrs = all_edges_indexed.select(\"len_como\").toPandas().values\n","\n"]},{"cell_type":"code","source":["# dst_indices = all_edges_indexed.select(\"dst_index\").toPandas().values\n","dst_indices = all_edges_indexed.select(\"dst_index\").rdd.flatMap(lambda x: x).collect()"],"metadata":{"id":"UTswA61n9Psy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# edge_index_person_to_diagnosis = torch.tensor([src_indices, dst_indices]).contiguous()\n","edge_index_person_to_diagnosis = torch.tensor([src_indices, dst_indices]).contiguous()\n","\n","print(edge_index_person_to_diagnosis)"],"metadata":{"id":"iqfKuq-0HD14"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2iP6wnYtDLqD"},"outputs":[],"source":["edge_index_diagnosis_to_person = torch.tensor([dst_indices, src_indices]).contiguous()\n","# edge_index_diagnosis_to_person = torch.tensor([dst_indices[:,0].tolist(), src_indices]).contiguous()\n","# print(edge_index_person_to_diagnosis)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRihpDJA9a-V"},"outputs":[],"source":["edge_attrs = all_edges_indexed.select(\"len_como\").rdd.map(lambda x: x[0]).collect()\n","# edge_attrs = all_edges_indexed.select(\"len_como\").toPandas().values[:,0].tolist()\n","\n","# Create edge attribute tensor\n","edge_attr = torch.tensor(edge_attrs, dtype=torch.float).unsqueeze(1)  # Unsqueeze to match the expected shape\n","print(edge_attr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbQB1tAtwryN"},"outputs":[],"source":["# prompt: transform the cohort_new so that personid is converted to indexes using id_to_idx. EPI is the label. Make sure the label is aligned with id in x_person. Load the label to y and prepare data to be loaded using dataloader\n","\n","# Create a UDF to map person IDs to indices for labels\n","map_person_id_for_label_udf = udf(lambda id_: id_to_idx.get(id_), IntegerType())\n","\n","# Apply the UDF to the 'personid' column of 'cohort_new' and select 'EPI' as label\n","cohort_new_indexed = cohort_new.withColumn(\"id_index\", map_person_id_for_label_udf(col(\"personid\"))).select('id_index','EPI')\n","\n","# align with x_person by doing sorting\n","cohort_new_indexed = cohort_new_indexed.orderBy('id_index')\n","# display(cohort_new_indexed.show(5))\n","\n","# Extract labels and convert to tensor\n","y = torch.tensor(cohort_new_indexed.select(\"EPI\").toPandas().values.astype(int), dtype=torch.long).squeeze()\n","\n","# Ensure labels are aligned with IDs in x_person\n","# (Assuming x_person is already sorted by person ID index)\n","# If not, you might need to sort both x_person and y by ID index\n","\n","\n","\n"]},{"cell_type":"code","source":["from torch_geometric.data import HeteroData, DataLoader\n","# Create HeteroData object\n","data1 = HeteroData()\n","data1.x = x_combined.to(torch.int64)\n","data1.edge_index = edge_index_person_to_diagnosis\n","data1.edge_attr = edge_attr\n","data1.y = y\n"],"metadata":{"id":"WplShTDSPJ_G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRiGHwWO_6OQ"},"outputs":[],"source":["data1 # 6661834"]},{"cell_type":"code","source":["data1.x"],"metadata":{"id":"XSAGTF97_q1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: write data1 into h5\n","\n","import h5py\n","\n","# Create an h5 file\n","with h5py.File('/content/drive/MyDrive/UCO/projects/epilepsy/EPI_heterodata_diag50_first.h5', 'w') as f:\n","  # Store node features\n","  f.create_dataset('x', data=data1.x.numpy())\n","\n","  # Store edge index\n","  f.create_dataset('edge_index', data=data1.edge_index.numpy())\n","\n","  # Store edge attributes\n","  f.create_dataset('edge_attr', data=data1.edge_attr.numpy())\n","\n","  # Store labels\n","  f.create_dataset('y', data=data1.y.numpy())\n"],"metadata":{"id":"bwLijaL0-xkA"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1Fxfb88Ds8X28tM47JJrP1XxpyjmUy60M","timestamp":1736223436354}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}